---
title: Tutorials
layout: default
---

# Tutorials

## Access

### How do I look at images?

We have developed our own lightweight Web-visualization tool called [NeuroDataViz](http://docs.neurodata.io/NeuroDataViz/).  It has been designed specifically to enable:  volumetric annotations, multi-spectral data, and time-series data.  Here are a few different examples:

* [kharris15apical](http://brainviz1.cs.jhu.edu/ndv/kharris15apical/em,ramon_test/3/507/469/90/) demonstrates annotation overlays and metadata capabilities
  * Image metadata is available via the *Project Info* button in the menubar.
  * From the menubar, select *Query*, then *kharris15apical / ramon_test*.
  * The blue cell in the center of the image has a corresponding ramon object. Clicking on the blue cell will bring up RAMON metadata. No other cells/objects have ramon objects.

* [aratome15c](http://brainviz1.cs.jhu.edu/ndv/project/Aratome15c_S17_W10/1/5818/527/0/) demonstrates multi-spectral color blending capabilities
  * play with opacity, brightness, and contrast
  * change the blend mode to enable different color blends

* [freeman14](http://viz.neurodata.io/freeman14/) demonstrates 3D time-series capabilities.

To see other data in NeuroDataViz, browse to our [projects](http://neurodata.io/projects) page, and select a dataset of interest.  Many of those datasets automatically link to NeuroDataViz.  Of note, they are not yet all currently centered, so you might have to move around a bit to find the data.

There are many other viewers out there, with complementary features.  The ones that we support are listed below in the 3rd party apps section.


### How do I look at Graphs?

We have developed [Graph Explorer]128.220.176.8/Graph-Explorer/shiny/), a [shiny app](https://www.shinyapps.io/) that allows you to upload any graph to both visualize and analyze it.  Other 3rd party graph visualization/analysis tools can also be used with any of our graphs.

### Use the Histogram Explorer tool?

The *Histogram Explorer* is available at [hx.neurodata.io](http://hx.neurodata.io). An *NDStore* dataset **Token** and **Channel** are required to load histograms and associated statistics. Public tokens and channels are available in Table 1, Table 2, and supplementary material.

#### 8-bit Datasets

Figure 2(b) was generated using Histogram Explorer. To setup the view shown in the figure, first visit [hx.neurodata.io](http://hx.neurodata.io). Enter **kharris15apical** in the Token box, and **em** in the Channel box. Press "Load Histogram" to load the histogram and relevant statistics.

#### 16-bit Datasets

The *Histogram Explorer* can also be used to determine an appropriate "image window" for 16-bit datasets. The image window sets a low bound and high bound for resampling a 16-bit dataset into an 8-bit image. Often, the low bound is set at around the 15% and 25% of the dataset intensity value, and the high bound is set at between 80% and 99%.

To use the tool to resample a dataset, first load a 16-bit token and channel into the Histogram Explorer. A good example token / channel combination is **Ex10R55** and **DAPI_1**. The *Ex10R55* dataset was collected by Weiler et al and published in Nature Scientific Data in 2014 [(Weiler et al., 2014)](http://www.nature.com/articles/sdata201446).

Immediately, it is apparent that the 16-bit dataset is using a small fraction of the 16-bit intensity space. The plotted histogram will appear blank. The plot is interactive, and the user can zoom in on a particular chunk of the histogram by clicking and dragging across.

At the bottom of the page is an image loaded from NDStore. The double slider immediately above the image adjusts the resampling from 16-bit space to 8-bit space. After moving the slider, click "*Apply Window to Image*" to see the results updated in the image below.

### Download some data to play with on my computer?

#### Images

- The easiest way to download data is via our [Download Web-form](http://neurodata.io/exchange/download). It presents two options:
  - "Ready-to-go" downloads provide frequently requested cubes as a PNG stack
  - If you know the token, channel, resolution, and bounding box for some data of interest, and you want less than 2 GB, you can type it in directly.  Information about each dataset can be found in the project info pages
(e.g., `openconnecto.me/ocp/ca/bock11/info/`)
- [ndio](http://docs.neurodata.io/ndio/) is our Python library that lets you get and put data into NeuroData.  As long as you are transfering < 1 TB of data, this should work fine.  An example ipython notebook can be found [here](https://github.com/openconnectome/ndio-demos/blob/master/Getting%20Started/Downloading%20a%20Volume.ipynb).  An example call is:

~~~
bock11_subvol = oo.get_cutout('bock11', 'image', 11000, 11100, 11000, 11100, 4000, 4010, resolution=3)
~~~

- [CAJAL](http://docs.neurodata.io/CAJAL/) is our MATLAB toolbox for NeuroData. It has all the functionality of ndio, and more.  It can also be used for transferring data up to about 1 TB.  A code snippet to download data can be found below.

~~~
%% Create OCP Interface and set tokens
oo = OCP();
oo.setServerLocation('http://openconnecto.me/');
oo.setImageToken('kasthuri11cc');
oo.setDefaultResolution(1);

xstart = 5500;
ystart = 6000;
zstart = 400;

% Build query to cutout IMAGE data.
q = OCPQuery(eOCPQueryType.imageDense);
q.setCutoutArgs([xstart xstart+500],...
                [ystart ystart+500],...
                [zstart zstart+5], 1);

% Download the data into a RAMONVolume.
cutout = oo.query(q);
~~~

- [Data API](http://docs.neurodata.io/open-connectome/api/data_api.html#get): you can also download data < 2 GB via using the RESTful calls directly.  An example call follows:

~~~
http://openconnecto.me/ocp/ca/kasthuri11cc/image/hdf5/1/5472,6496/8712,9736/1000,1100/
~~~

#### Graphs

Downloading graphs is easy, simply go to our [graph download](http://openconnecto.me/graph-services/download/) Web-page.  From there, you can select the format (eg, MATLAB, graphml, etc.), and even scale for the human MR derived graphs.  Many other graph services are available from there as well.


### Show some image data to labmates/colleagues?

#### Images

- web-form < 200 GB
- CAJAL < 1 TB
- ndio < 1 TB
- auto-ingest < 2 TB https://github.com/openconnectome/open-connectome/blob/master/ingest/autoingest/verifyjson.py
- (s)ftp/(s)http/open-s3: arbitrary size https://github.com/openconnectome/open-connectome/tree/master/ingest/

#### Graphs

-----

## Computer Vision

#### Stitch and align into volume?

#### Color correct a volume?

- 2D
- 3D


#### Manually annotate some data?

- CATMAID
- knossos
- VAST
- Reconstruct
- ITK-SNAP
â€¦.

#### Share annotations (RAMONify)?

csv ingest script (with spec)

#### Proofread and error correct metadata?

- ocpviz: edit metadata
- parity

#### Train my machine vision algorithm on some data?

Training paradigms are different for each use-case.  We provide examples for a random forest training example and a paradigm using ilastik.  Training is typically conducted using standalone functions (v. a distributed LONI approach)

- vesicle RF example
- ilastik example

#### Assess the performance of algorithms?

example (in matlab), of making ROC curves on held out data from a separate volume
example in loni sweeping over a set of hyperparameters

#### Run my machine vision algorithm at scale?

- LONI stuff

-----

## Analysis

### FlashGraph

[FlashGraph](http://www.flashgraph.net/) is an extremely fast, parallelized and highly scalable graph processing engine. It can operate in *Semi-External Memory* mode, meaning the RAM requirements are vastly reduced; this allows users to process graphs with billions of vertices even on personal computers.

FlashGraph is written in `C++` for speed, but has an `R` interface, [FlashGraphR](https://github.com/icoming/FlashGraph/wiki/FlashGraphR-Quick-Start-Guide). Documentaion on how to install and use FlashGraph can be found [here](https://github.com/icoming/FlashGraph/wiki).

### m2g

#### How do I generate multiscale graphs from MRI data?

We have developed a one-click multi-modal MRI processing pipeline called [m2g](http://m2g.io) which estimates structural graphs from diffusion MRI. The pipeline and its sourcecode can be found on [github](https://github.com/openconnectome/m2g), and full documentation exists on our [m2g webpage](http://m2g.io).  

We offer several ways to interact with m2g:

- Our [Webservice](http://openconnecto.me/graph-services/c4/) allows users to upload a brain image and have a connectome emailed to them after it has been generated.
- A [Standalone Cluster](http://m2g.io/sphinx/services.html#using-m2g-on-captive-cluster) exists on which we process large volumes of data with collaborators.
- An [Amazon EC2](http://m2g.io/sphinx/services.html#using-m2g-in-an-ami) machine image with the NeuroData stack and the m2g pipeline installed is publicly available for use.
- Our [graph service engine](http://mrbrain.cs.jhu.edu/graph-services/welcome/) hosts graph data of a variety of species and scales, as well as enables downsampling and type conversion of graphs.
- Pipeline derivatives and graphs may also be found in our public s3 bucket. Contact [support](mailto:support@neurodata.io) to gain access.

#### How do I know that the graphs I made are reliable?

We assess the quality of our graphs using a metric called reliability. Several test-retest (TRT) datasets have been chosen as benchmarks for performance, and a detailed explanation of how they were evaluated and their performance can be found on our [validation page](http://m2g.io/tutorials/validation.html).

#### Where can I find public data so that I may get started?

We have processed and share several publicly available datasets on our s3 bucket. Look at our [available data](http://m2g.io/tutorials/available_data.html) page for information about reading and accessing this data.

#### How long does it take to run the pipeline?

In addition to recording reliability performance on our benchmark datasets (see above), we record [timing](http://m2g.io/tutorials/validation.html) as well. Currently, on our cluster, the pipeline takes approximately 6 hours/subject/core when running at saturation to produce small graphs.

### Query the scene?

- CAJAL
- ndio

### Estimate statistical properties?

-----

## Other stuff

### Computing Resources

### Third Party Tools Integration Status

#### ITK Snap

Works in manno for annotation purposes (see http://docs.neurodata.io/manno).

#### VAST

VAST was developed by Daniel Berger at MIT and Harvard and can pull image data from OpenConnectome project for annotating EM volumes.  Details are available here:  https://software.rc.fas.harvard.edu/lichtman/vast/  (please also see details in the linked manual).

#### Others
- bigdataviewer:  Broke when we switched to channel interface.  Stephan is fixing.
catmaid:  [ask alex]
- vaa3d:  Nathan and Dean did this.  Code broke when we switched to channel interface.
- knossos:  Kunal wrote a backend so that they could pull data.  Status unknown.  No import capability.
- viking:   Kunal wrote a backend so that they could pull data.  Status unknown.  No import capability.
- neutube:  none yet
- janelia-workstation:  ??
- reconstruct:  Alex has some scripts.
- VAST:  can pull data from neurodata
- [Gephi](http://gephi.github.io/): can load any graph downloaded from [here](http://openconnecto.me/graph-services/download/)
